# Competitor Intelligence Agent - Status Report

**Datum:** 13. November 2025  
**Projekt:** Mi42 Market Intelligence Platform

---

## üìä Aktueller Stand

### ‚úÖ Was ist implementiert:

**1. Agent-Konfiguration (`server/agents.ts`)**
- ‚úÖ 7 Agent-Typen definiert:
  - `market_analyst` - Marktanalyse
  - `trend_scout` - Trend-Erkennung
  - `survey_assistant` - Umfrage-Analyse
  - `strategy_advisor` - Strategieberatung
  - `demand_forecasting` - Nachfrageprognose ‚ö†Ô∏è
  - `project_intelligence` - Bauprojekt-Intelligence ‚ö†Ô∏è
  - `pricing_strategy` - Preisstrategie ‚ö†Ô∏è

**2. LLM-Integration**
- ‚úÖ Open WebUI API verbunden (https://maxproxy.bl2020.com)
- ‚úÖ Model: `gpt-oss:120b`
- ‚úÖ Konfigurierbar per User/Agent-Typ in `model_configs` Tabelle
- ‚úÖ Fallback auf Gemini 2.5 Flash wenn Open WebUI nicht verf√ºgbar

**3. Datenbank-Schema**
- ‚úÖ `agent_tasks` Tabelle mit 8 Agent-Typen (inkl. `competitor_intelligence`)
- ‚úÖ `briefings` Tabelle f√ºr Ergebnisse
- ‚úÖ `company_profiles` f√ºr Unternehmensanalysen
- ‚úÖ `model_configs` f√ºr LLM-Konfiguration

**4. Onboarding-Integration**
- ‚úÖ Automatische Erstellung von 7 Analysen nach Registration
- ‚úÖ Company-Profile-Analyse basierend auf Domain

---

## ‚ùå Was NICHT funktioniert:

### Problem 1: Agent-Execution schl√§gt fehl

**Fehler:** `TypeError: Cannot read properties of undefined (reading 'systemPrompt')`

**Ursache:** 
- Die 3 neuen Agent-Typen (`demand_forecasting`, `project_intelligence`, `pricing_strategy`) haben **keine Konfiguration** in `AGENT_CONFIGS`
- Code versucht `config.systemPrompt` zu lesen, aber `config` ist `undefined`

**Betroffene Agents:**
- ‚ùå Demand Forecasting (Nachfrageprognose)
- ‚ùå Project Intelligence (Bauprojekt-Analyse)
- ‚ùå Pricing Strategy (Preisstrategie)

**Funktionierende Agents:**
- ‚úÖ Market Analyst
- ‚úÖ Trend Scout
- ‚úÖ Survey Assistant
- ‚úÖ Strategy Advisor

### Problem 2: Competitor Intelligence Agent fehlt komplett

**Status:** 
- ‚úÖ In Datenbank-Schema definiert (`competitor_intelligence`)
- ‚ùå **KEINE Konfiguration** in `AGENT_CONFIGS`
- ‚ùå **KEINE System-Prompts**
- ‚ùå **NICHT in Onboarding** integriert

---

## üîç LLM-Vergleich: Open WebUI vs. OpenAI

### Aktuelles Setup (Open WebUI):

**Model:** `gpt-oss:120b`  
**Endpoint:** https://maxproxy.bl2020.com/api/chat/completions  
**API Key:** `sk-bd621b0666474be1b054b3c5360b3cef`

**Vorteile:**
- ‚úÖ Eigenes LLM, keine externen Kosten
- ‚úÖ Datenschutz (keine Daten an OpenAI)
- ‚úÖ Unbegrenzte Requests

**Nachteile:**
- ‚ùå Qualit√§t m√∂glicherweise schlechter als GPT-4
- ‚ùå Keine Structured Outputs (JSON Schema)
- ‚ùå Langsamer als OpenAI

### OpenAI Alternative:

**Models verf√ºgbar:**
- `gpt-4o` - Beste Qualit√§t, teuer ($2.50/1M input tokens)
- `gpt-4o-mini` - Gute Qualit√§t, g√ºnstig ($0.15/1M input tokens)
- `o1-mini` - Reasoning-optimiert ($3.00/1M input tokens)

**Vorteile:**
- ‚úÖ Beste Qualit√§t f√ºr komplexe Analysen
- ‚úÖ Structured Outputs (JSON Schema) funktioniert perfekt
- ‚úÖ Schneller
- ‚úÖ Reasoning-Capabilities (o1-mini)

**Nachteile:**
- ‚ùå Kosten pro Request
- ‚ùå Datenschutz-Bedenken
- ‚ùå Rate Limits

---

## üß™ Vergleichs-Test-Plan

### Option 1: A/B-Test im Code

**Implementierung:**
1. OpenAI API Key als Umgebungsvariable hinzuf√ºgen
2. `model_configs` Tabelle erweitern um `provider` Feld
3. F√ºr jeden Agent 2 Konfigurationen erstellen:
   - `market_analyst_openwebui` ‚Üí `gpt-oss:120b`
   - `market_analyst_openai` ‚Üí `gpt-4o-mini`
4. Beide Agents parallel ausf√ºhren f√ºr dieselbe Aufgabe
5. Ergebnisse vergleichen (Qualit√§t, Geschwindigkeit, Kosten)

**Code-√Ñnderungen:**
```typescript
// server/agents.ts
const modelConfig = await getModelConfig(userId, agentType);
const provider = modelConfig?.modelProvider || 'open_webui';
const model = modelConfig?.modelName || (provider === 'openai' ? 'gpt-4o-mini' : 'gpt-oss:120b');

const response = await invokeLLM({
  modelProvider: provider,
  modelName: model,
  messages: [
    { role: "system", content: config.systemPrompt },
    { role: "user", content: contextPrompt },
  ],
});
```

### Option 2: Manual Testing

**Prozess:**
1. Erstellen Sie 2 Test-User:
   - User A: Open WebUI (`gpt-oss:120b`)
   - User B: OpenAI (`gpt-4o-mini`)
2. Beide User stellen dieselbe Aufgabe:
   - "Analysiere den Wettbewerber Heidelberg Materials im Bereich Zement"
3. Vergleichen Sie:
   - **Qualit√§t:** Detailtiefe, Genauigkeit, Relevanz
   - **Geschwindigkeit:** Response-Zeit
   - **Kosten:** Token-Verbrauch √ó Preis

### Option 3: Benchmark-Suite

**Test-Cases:**
1. **Einfache Analyse:** "Liste die Top 5 Zementhersteller in Deutschland"
2. **Komplexe Analyse:** "Analysiere die Preisstrategie von Heidelberg Materials im Vergleich zu Holcim"
3. **Strukturierte Daten:** "Extrahiere Produktportfolio, M√§rkte und Wettbewerber von heidelbergmaterials.com"
4. **Reasoning:** "Welche Markteintrittsstrategie empfiehlst du f√ºr einen neuen Baustoffhersteller in S√ºddeutschland?"

**Metriken:**
- Qualit√§t (1-10 Skala, manuell bewertet)
- Response-Zeit (Sekunden)
- Token-Verbrauch
- Kosten pro Request
- Fehlerrate

---

## üéØ Empfehlungen

### Kurzfristig (diese Woche):

1. **Competitor Intelligence Agent implementieren**
   - System-Prompt schreiben
   - In `AGENT_CONFIGS` hinzuf√ºgen
   - In Onboarding integrieren

2. **Fehlende Agent-Konfigurationen fixen**
   - `demand_forecasting` ‚úÖ (bereits vorhanden, aber nicht deployed)
   - `project_intelligence` ‚úÖ (bereits vorhanden, aber nicht deployed)
   - `pricing_strategy` ‚úÖ (bereits vorhanden, aber nicht deployed)

3. **OpenAI Integration vorbereiten**
   - API Key als Umgebungsvariable hinzuf√ºgen
   - `invokeLLM` erweitern um OpenAI-Support
   - Test-User mit OpenAI-Konfiguration erstellen

### Mittelfristig (n√§chste 2 Wochen):

4. **A/B-Test durchf√ºhren**
   - 10 Test-Aufgaben definieren
   - Beide LLMs parallel testen
   - Ergebnisse dokumentieren

5. **Hybrid-Strategie**
   - Einfache Aufgaben ‚Üí Open WebUI (kostenlos)
   - Komplexe Analysen ‚Üí OpenAI (bessere Qualit√§t)
   - User kann w√§hlen (Premium-Feature)

---

## üí° N√§chste Schritte

**Soll ich:**

1. ‚úÖ **Competitor Intelligence Agent komplett implementieren** (System-Prompt, Konfiguration, Tests)

2. ‚úÖ **OpenAI Integration hinzuf√ºgen** (API Key, Code-√Ñnderungen, Test-Setup)

3. ‚úÖ **A/B-Test durchf√ºhren** (10 Test-Cases, beide LLMs vergleichen, Dokumentation)

**Oder alle 3 Schritte nacheinander?**

---

## üìã Technische Details

### Aktuelle LLM-Konfiguration:

```yaml
OPEN_WEBUI_API_URL: https://maxproxy.bl2020.com/api/chat/completions
OPEN_WEBUI_API_KEY: sk-bd621b0666474be1b054b3c5360b3cef
OPEN_WEBUI_MODEL: gpt-oss:120b
```

### Ben√∂tigte OpenAI-Konfiguration:

```yaml
OPENAI_API_KEY: <IHR_API_KEY>
OPENAI_MODEL: gpt-4o-mini  # oder gpt-4o f√ºr beste Qualit√§t
```

### Model-Preise (OpenAI):

| Model | Input | Output | Qualit√§t |
|-------|-------|--------|----------|
| gpt-4o | $2.50/1M | $10.00/1M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| gpt-4o-mini | $0.15/1M | $0.60/1M | ‚≠ê‚≠ê‚≠ê‚≠ê |
| o1-mini | $3.00/1M | $12.00/1M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Reasoning) |

**Gesch√§tzte Kosten pro Analyse:**
- Einfache Analyse (500 tokens): ~$0.0003 (gpt-4o-mini)
- Komplexe Analyse (2000 tokens): ~$0.0012 (gpt-4o-mini)
- **100 Analysen/Tag = ~$0.12/Tag = $3.60/Monat**

---

**Fazit:** OpenAI ist **sehr g√ºnstig** f√ºr die Qualit√§t, die Sie bekommen. Ich empfehle einen Hybrid-Ansatz:
- **Open WebUI** f√ºr einfache Aufgaben (kostenlos)
- **OpenAI gpt-4o-mini** f√ºr wichtige Analysen (bessere Qualit√§t, minimal Kosten)
